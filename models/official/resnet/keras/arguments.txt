Runs a ResNet model on the ImageNet dataset.
flags:

absl.app:
  -?,--[no]help: show this help
    (default: 'false')
  --[no]helpfull: show full help
    (default: 'false')
  -h,--[no]helpshort: show this help
    (default: 'false')
  --[no]helpxml: like --helpfull, but generates XML output
    (default: 'false')
  --[no]only_check_args: Set to true to validate args and exit.
    (default: 'false')
  --[no]pdb_post_mortem: Set to true to handle uncaught exceptions with PDB post
    mortem.
    (default: 'false')
  --profile_file: Dump profile information to a file (for python -m pstats).
    Implies --run_with_profiling.
  --[no]run_with_pdb: Set to true for PDB debug mode
    (default: 'false')
  --[no]run_with_profiling: Set to true for profiling the script. Execution will
    be slower, and the output format might change over time.
    (default: 'false')
  --[no]use_cprofile_for_profiling: Use cProfile instead of the profile module
    for profiling. This has no effect unless --run_with_profiling is set.
    (default: 'true')

absl.logging:
  --[no]alsologtostderr: also log to stderr?
    (default: 'false')
  --log_dir: directory to write logfiles into
    (default: '')
  --[no]logtostderr: Should only log to stderr?
    (default: 'false')
  --[no]showprefixforinfo: If False, do not prepend prefix to info messages when
    it's logged to stderr, --verbosity is set to INFO level, and python logging
    is used.
    (default: 'true')
  --stderrthreshold: log messages at this level, or more severe, to stderr in
    addition to the logfile.  Possible values are 'debug', 'info', 'warning',
    'error', and 'fatal'.  Obsoletes --alsologtostderr. Using --alsologtostderr
    cancels the effect of this flag. Please also note that this flag is subject
    to --verbosity and requires logfile not be stderr.
    (default: 'fatal')
  -v,--verbosity: Logging verbosity level. Messages logged at this level or
    lower will be included. Set to 1 for debug logging. If the flag was not set
    or supplied, the value will be changed from the default of -1 (warning) to 0
    (info) after flags are parsed.
    (default: '-1')
    (an integer)

absl.testing.absltest:
  --test_random_seed: Random seed for testing. Some test frameworks may change
    the default value of this flag between runs, so it is not appropriate for
    seeding probabilistic tests.
    (default: '301')
    (an integer)
  --test_randomize_ordering_seed: If positive, use this as a seed to randomize
    the execution order for test cases. If "random", pick a random seed to use.
    If 0 or not set, do not randomize test case execution order. This flag also
    overrides the TEST_RANDOMIZE_ORDERING_SEED environment variable.
  --test_srcdir: Root of directory tree where source files live
    (default: '')
  --test_tmpdir: Directory for temporary testing files
    (default: '/tmp/absl_testing')
  --xml_output_file: File to store XML test results
    (default: '')

official.resnet.keras.keras_common:
  --[no]batchnorm_spatial_persistent: Enable the spacial persistent mode for
    CuDNN batch norm kernel.
    (default: 'true')
  --[no]data_delay_prefetch: Add a small delay in tf.data prefetch to prioritize
    memory copy of other tensors over the data minibatch for the (T+1)th step.
    It should help improve performance using EagerIterator and function. The
    codepath when enabling this feature is experimental and will be removed once
    the corresponding performance features are fully supported in TensorFlow.
    (default: 'false')
  --[no]enable_eager: Enable eager?
    (default: 'false')
  --[no]enable_get_next_as_optional: Enable get_next_as_optional behavior in
    DistributedIterator.
    (default: 'false')
  --[no]enable_grappler_layout_optimizer: Enable Grappler layout optimizer.
    Currently Grappler can de-optimize fp16 graphs byt forcing NCHW layout for
    all convolutions and batch normalizations, and this flag allows to disable
    it.
    (default: 'true')
  --[no]enable_tensorboard: Whether to enable Tensorboard callback.
    (default: 'false')
  --[no]explicit_gpu_placement: If not using distribution strategy, explicitly
    set device scope for the Keras training loop.
    (default: 'false')
  --profile_steps: Save profiling data to model dir at given range of steps. The
    value must be a comma separated pair of positive integers, specifying the
    first and last step to profile. For example, "--profile_steps=2,4" triggers
    the profiler to process 3 steps, starting from the 2nd step. Note that
    profiler has a non-trivial performance overhead, and the output file can be
    gigantic if profiling many steps.
  --[no]report_accuracy_metrics: Report metrics during training and evaluation.
    (default: 'true')
  --[no]set_learning_phase_to_train: If skip eval, also set Keras learning phase
    to 1 (training).
    (default: 'true')
  --[no]skip_eval: Skip evaluation?
    (default: 'false')
  --train_steps: The number of steps to run for training. If it is larger than #
    batches per epoch, then use # batches per epoch. When this flag is set, only
    one epoch is going to run for training.
    (an integer)
  --[no]use_tensor_lr: Use learning rate tensor instead of a callback.
    (default: 'false')
  --[no]use_trivial_model: Whether to use a trivial Keras model.
    (default: 'false')

official.utils.flags._base:
  -bs,--batch_size:
    Batch size for training and evaluation. When using multiple gpus, this is
    the
    global batch size for all devices. For example, if the batch size is 32 and
    there are 4 GPUs, each GPU will get 8 examples on each step.
    (default: '32')
    (an integer)
  --[no]clean:
    If set, model_dir will be removed if it exists.
    (default: 'false')
  -dd,--data_dir:
    The location of the input data.
    (default: '/tmp')
  -ds,--distribution_strategy:
    The Distribution Strategy to use for training. Accepted values are 'off',
    'default', 'one_device', 'mirrored', 'parameter_server', 'collective', case
    insensitive. 'off' means not to use Distribution Strategy; 'default' means
    to
    choose from `MirroredStrategy` or `OneDeviceStrategy` according to the
    number of
    GPUs.
    (default: 'default')
  -ebe,--epochs_between_evals:
    The number of training epochs to run between evaluations.
    (default: '1')
    (an integer)
  -ed,--export_dir:
    If set, a SavedModel serialization of the model will be exported to this
    directory at the end of training. See the README for more details and
    relevant
    links.
  -hk,--hooks:
    A list of (case insensitive) strings to specify the names of training hooks.
    Hook:
    loggingtensorhook
    profilerhook
    examplespersecondhook
    loggingmetrichook
    stepcounterhook
    Example: `--hooks ProfilerHook,ExamplesPerSecondHook`
    See official.utils.logs.hooks_helper for details.
    (default: 'LoggingTensorHook')
    (a comma separated list)
  -md,--model_dir:
    The location of the model checkpoint files.
    (default: '/tmp')
  -ng,--num_gpus:
    How many GPUs to use at each worker with the DistributionStrategies API. The
    default is 1.
    (default: '1')
    (an integer)
  --[no]run_eagerly: Run the model op by op without building a model function.
    (default: 'false')
  -st,--stop_threshold:
    If passed, training will stop at the earlier of train_epochs and when the
    evaluation metric is  greater than or equal to stop_threshold.
    (a number)
  -te,--train_epochs:
    The number of epochs used to train.
    (default: '90')
    (an integer)

official.utils.flags._benchmark:
  -bld,--benchmark_log_dir:
    The location of the benchmark logging.
  --benchmark_logger_type:
    <BaseBenchmarkLogger|BenchmarkFileLogger|BenchmarkBigQueryLogger>:
    The type of benchmark logger to use. Defaults to using BaseBenchmarkLogger
    which logs to STDOUT. Different loggers will require other flags to be able
    to
    work.
    (default: 'BaseBenchmarkLogger')
  -bti,--benchmark_test_id:
    The unique test ID of the benchmark run. It could be the combination of key
    parameters. It is hardware independent and could be used compare the
    performance
    between different test runs. This flag is designed for human consumption,
    and
    does not have any impact within the system.
  -bds,--bigquery_data_set:
    The Bigquery dataset name where the benchmark will be uploaded.
    (default: 'test_benchmark')
  -bmt,--bigquery_metric_table:
    The Bigquery table name where the benchmark metric information will be
    uploaded.
    (default: 'benchmark_metric')
  -brst,--bigquery_run_status_table:
    The Bigquery table name where the benchmark run status information will be
    uploaded.
    (default: 'benchmark_run_status')
  -brt,--bigquery_run_table:
    The Bigquery table name where the benchmark run information will be
    uploaded.
    (default: 'benchmark_run')
  -gp,--gcp_project:
    The GCP project name where the benchmark will be uploaded.
  --log_steps: For every log_steps, we log the timing information such as
    examples per second. Besides, for every log_steps, we store the timestamp of
    a batch end.
    (default: '100')
    (an integer)

official.utils.flags._misc:
  -df,--data_format: <channels_first|channels_last>:
    A flag to override the data format used in the model. channels_first
    provides a
    performance boost on GPU but is not always compatible with CPU. If left
    unspecified, the data format will be chosen automatically based on whether
    TensorFlow was built for CPU or GPU.

official.utils.flags._performance:
  -ara,--all_reduce_alg:
    Defines the algorithm to use for performing all-reduce.When specified with
    MirroredStrategy for single worker, this controls
    tf.contrib.distribute.AllReduceCrossTowerOps.  When specified with
    MultiWorkerMirroredStrategy, this controls
    tf.distribute.experimental.CollectiveCommunication; valid options are `ring`
    and
    `nccl`.
  --datasets_num_private_threads:
    Number of threads for a private threadpool created for alldatasets
    computation..
    (an integer)
  -dt,--dtype: <fp16|fp32>:
    The TensorFlow datatype used for calculations. Variables may be cast to a
    higher precision on a case-by-case basis for numerical stability.
    (default: 'fp32')
  --[no]enable_xla: Whether to enable XLA auto jit compilation
    (default: 'false')
  --[no]force_v2_in_keras_compile: Forces the use of run_distribued path even if
    notusing a `strategy`. This is not the same
    as`tf.distribute.OneDeviceStrategy`
    (default: 'false')
  -inter,--inter_op_parallelism_threads:
    Number of inter_op_parallelism_threads to use for CPU. See TensorFlow
    config.proto for details.
    (default: '0')
    (an integer)
  -intra,--intra_op_parallelism_threads:
    Number of intra_op_parallelism_threads to use for CPU. See TensorFlow
    config.proto for details.
    (default: '0')
    (an integer)
  -ls,--loss_scale:
    The amount to scale the loss by when the model is run. This can be an
    int/float
    or the string 'dynamic'. Before gradients are computed, the loss is
    multiplied
    by the loss scale, making all gradients loss_scale times larger. To adjust
    for
    this, gradients are divided by the loss scale before being applied to
    variables.
    This is mathematically equivalent to training without a loss scale, but the
    loss
    scale helps avoid some intermediate gradients from underflowing to zero. If
    not
    provided the default for fp16 is 128 and 1 for all other dtypes. The string
    'dynamic' can be used to dynamically determine the optimal loss scale during
    training, but currently this significantly slows down performance
  -mts,--max_train_steps:
    The model will stop training if the global_step reaches this value. If not
    set,
    training will run until the specified number of epochs have run as usual. It
    is
    generally recommended to set --train_epochs=1 when using this flag.
    (an integer)
  --num_packs:
    Sets `num_packs` in the cross device ops used in MirroredStrategy.  For
    details, see tf.distribute.NcclAllReduce.
    (default: '1')
    (an integer)
  -pgtc,--per_gpu_thread_count:
    The number of threads to use for GPU. Only valid when tf_gpu_thread_mode is
    not
    global.
    (default: '0')
    (an integer)
  --[no]tf_data_experimental_slack:
    Whether to enable tf.data's `experimental_slack` option.
    (default: 'false')
  -gt_mode,--tf_gpu_thread_mode:
    Whether and how the GPU device uses its own threadpool.
  -synth,--[no]use_synthetic_data:
    If set, use fake data (zeroes) instead of a real dataset. This mode is
    useful
    for performance debugging, as it removes input processing steps, but will
    not
    learn anything.
    (default: 'false')

tensorflow.python.ops.parallel_for.pfor:
  --[no]op_conversion_fallback_to_while_loop: If true, falls back to using a
    while loop for ops for which a converter is not defined.
    (default: 'false')

absl.flags:
  --flagfile: Insert flag definitions from the given file into the command line.
    (default: '')
  --undefok: comma-separated list of flag names that it is okay to specify on
    the command line even if the program does not define a flag with that name.
    IMPORTANT: flags in this list that have arguments MUST use the --flag=value
    format.
    (default: '')
Runs a ResNet model on the ImageNet dataset.
flags:

absl.app:
  -?,--[no]help: show this help
    (default: 'false')
  --[no]helpfull: show full help
    (default: 'false')
  -h,--[no]helpshort: show this help
    (default: 'false')
  --[no]helpxml: like --helpfull, but generates XML output
    (default: 'false')
  --[no]only_check_args: Set to true to validate args and exit.
    (default: 'false')
  --[no]pdb_post_mortem: Set to true to handle uncaught exceptions with PDB post
    mortem.
    (default: 'false')
  --profile_file: Dump profile information to a file (for python -m pstats).
    Implies --run_with_profiling.
  --[no]run_with_pdb: Set to true for PDB debug mode
    (default: 'false')
  --[no]run_with_profiling: Set to true for profiling the script. Execution will
    be slower, and the output format might change over time.
    (default: 'false')
  --[no]use_cprofile_for_profiling: Use cProfile instead of the profile module
    for profiling. This has no effect unless --run_with_profiling is set.
    (default: 'true')

absl.logging:
  --[no]alsologtostderr: also log to stderr?
    (default: 'false')
  --log_dir: directory to write logfiles into
    (default: '')
  --[no]logtostderr: Should only log to stderr?
    (default: 'false')
  --[no]showprefixforinfo: If False, do not prepend prefix to info messages when
    it's logged to stderr, --verbosity is set to INFO level, and python logging
    is used.
    (default: 'true')
  --stderrthreshold: log messages at this level, or more severe, to stderr in
    addition to the logfile.  Possible values are 'debug', 'info', 'warning',
    'error', and 'fatal'.  Obsoletes --alsologtostderr. Using --alsologtostderr
    cancels the effect of this flag. Please also note that this flag is subject
    to --verbosity and requires logfile not be stderr.
    (default: 'fatal')
  -v,--verbosity: Logging verbosity level. Messages logged at this level or
    lower will be included. Set to 1 for debug logging. If the flag was not set
    or supplied, the value will be changed from the default of -1 (warning) to 0
    (info) after flags are parsed.
    (default: '-1')
    (an integer)

absl.testing.absltest:
  --test_random_seed: Random seed for testing. Some test frameworks may change
    the default value of this flag between runs, so it is not appropriate for
    seeding probabilistic tests.
    (default: '301')
    (an integer)
  --test_randomize_ordering_seed: If positive, use this as a seed to randomize
    the execution order for test cases. If "random", pick a random seed to use.
    If 0 or not set, do not randomize test case execution order. This flag also
    overrides the TEST_RANDOMIZE_ORDERING_SEED environment variable.
  --test_srcdir: Root of directory tree where source files live
    (default: '')
  --test_tmpdir: Directory for temporary testing files
    (default: '/tmp/absl_testing')
  --xml_output_file: File to store XML test results
    (default: '')

official.resnet.keras.keras_common:
  --[no]batchnorm_spatial_persistent: Enable the spacial persistent mode for
    CuDNN batch norm kernel.
    (default: 'true')
  --[no]data_delay_prefetch: Add a small delay in tf.data prefetch to prioritize
    memory copy of other tensors over the data minibatch for the (T+1)th step.
    It should help improve performance using EagerIterator and function. The
    codepath when enabling this feature is experimental and will be removed once
    the corresponding performance features are fully supported in TensorFlow.
    (default: 'false')
  --[no]enable_eager: Enable eager?
    (default: 'false')
  --[no]enable_get_next_as_optional: Enable get_next_as_optional behavior in
    DistributedIterator.
    (default: 'false')
  --[no]enable_grappler_layout_optimizer: Enable Grappler layout optimizer.
    Currently Grappler can de-optimize fp16 graphs byt forcing NCHW layout for
    all convolutions and batch normalizations, and this flag allows to disable
    it.
    (default: 'true')
  --[no]enable_tensorboard: Whether to enable Tensorboard callback.
    (default: 'false')
  --[no]explicit_gpu_placement: If not using distribution strategy, explicitly
    set device scope for the Keras training loop.
    (default: 'false')
  --profile_steps: Save profiling data to model dir at given range of steps. The
    value must be a comma separated pair of positive integers, specifying the
    first and last step to profile. For example, "--profile_steps=2,4" triggers
    the profiler to process 3 steps, starting from the 2nd step. Note that
    profiler has a non-trivial performance overhead, and the output file can be
    gigantic if profiling many steps.
  --[no]report_accuracy_metrics: Report metrics during training and evaluation.
    (default: 'true')
  --[no]set_learning_phase_to_train: If skip eval, also set Keras learning phase
    to 1 (training).
    (default: 'true')
  --[no]skip_eval: Skip evaluation?
    (default: 'false')
  --train_steps: The number of steps to run for training. If it is larger than #
    batches per epoch, then use # batches per epoch. When this flag is set, only
    one epoch is going to run for training.
    (an integer)
  --[no]use_tensor_lr: Use learning rate tensor instead of a callback.
    (default: 'false')
  --[no]use_trivial_model: Whether to use a trivial Keras model.
    (default: 'false')

official.utils.flags._base:
  -bs,--batch_size:
    Batch size for training and evaluation. When using multiple gpus, this is
    the
    global batch size for all devices. For example, if the batch size is 32 and
    there are 4 GPUs, each GPU will get 8 examples on each step.
    (default: '32')
    (an integer)
  --[no]clean:
    If set, model_dir will be removed if it exists.
    (default: 'false')
  -dd,--data_dir:
    The location of the input data.
    (default: '/tmp')
  -ds,--distribution_strategy:
    The Distribution Strategy to use for training. Accepted values are 'off',
    'default', 'one_device', 'mirrored', 'parameter_server', 'collective', case
    insensitive. 'off' means not to use Distribution Strategy; 'default' means
    to
    choose from `MirroredStrategy` or `OneDeviceStrategy` according to the
    number of
    GPUs.
    (default: 'default')
  -ebe,--epochs_between_evals:
    The number of training epochs to run between evaluations.
    (default: '1')
    (an integer)
  -ed,--export_dir:
    If set, a SavedModel serialization of the model will be exported to this
    directory at the end of training. See the README for more details and
    relevant
    links.
  -hk,--hooks:
    A list of (case insensitive) strings to specify the names of training hooks.
    Hook:
    loggingtensorhook
    profilerhook
    examplespersecondhook
    loggingmetrichook
    stepcounterhook
    Example: `--hooks ProfilerHook,ExamplesPerSecondHook`
    See official.utils.logs.hooks_helper for details.
    (default: 'LoggingTensorHook')
    (a comma separated list)
  -md,--model_dir:
    The location of the model checkpoint files.
    (default: '/tmp')
  -ng,--num_gpus:
    How many GPUs to use at each worker with the DistributionStrategies API. The
    default is 1.
    (default: '1')
    (an integer)
  --[no]run_eagerly: Run the model op by op without building a model function.
    (default: 'false')
  -st,--stop_threshold:
    If passed, training will stop at the earlier of train_epochs and when the
    evaluation metric is  greater than or equal to stop_threshold.
    (a number)
  -te,--train_epochs:
    The number of epochs used to train.
    (default: '90')
    (an integer)

official.utils.flags._benchmark:
  -bld,--benchmark_log_dir:
    The location of the benchmark logging.
  --benchmark_logger_type:
    <BaseBenchmarkLogger|BenchmarkFileLogger|BenchmarkBigQueryLogger>:
    The type of benchmark logger to use. Defaults to using BaseBenchmarkLogger
    which logs to STDOUT. Different loggers will require other flags to be able
    to
    work.
    (default: 'BaseBenchmarkLogger')
  -bti,--benchmark_test_id:
    The unique test ID of the benchmark run. It could be the combination of key
    parameters. It is hardware independent and could be used compare the
    performance
    between different test runs. This flag is designed for human consumption,
    and
    does not have any impact within the system.
  -bds,--bigquery_data_set:
    The Bigquery dataset name where the benchmark will be uploaded.
    (default: 'test_benchmark')
  -bmt,--bigquery_metric_table:
    The Bigquery table name where the benchmark metric information will be
    uploaded.
    (default: 'benchmark_metric')
  -brst,--bigquery_run_status_table:
    The Bigquery table name where the benchmark run status information will be
    uploaded.
    (default: 'benchmark_run_status')
  -brt,--bigquery_run_table:
    The Bigquery table name where the benchmark run information will be
    uploaded.
    (default: 'benchmark_run')
  -gp,--gcp_project:
    The GCP project name where the benchmark will be uploaded.
  --log_steps: For every log_steps, we log the timing information such as
    examples per second. Besides, for every log_steps, we store the timestamp of
    a batch end.
    (default: '100')
    (an integer)

official.utils.flags._misc:
  -df,--data_format: <channels_first|channels_last>:
    A flag to override the data format used in the model. channels_first
    provides a
    performance boost on GPU but is not always compatible with CPU. If left
    unspecified, the data format will be chosen automatically based on whether
    TensorFlow was built for CPU or GPU.

official.utils.flags._performance:
  -ara,--all_reduce_alg:
    Defines the algorithm to use for performing all-reduce.When specified with
    MirroredStrategy for single worker, this controls
    tf.contrib.distribute.AllReduceCrossTowerOps.  When specified with
    MultiWorkerMirroredStrategy, this controls
    tf.distribute.experimental.CollectiveCommunication; valid options are `ring`
    and
    `nccl`.
  --datasets_num_private_threads:
    Number of threads for a private threadpool created for alldatasets
    computation..
    (an integer)
  -dt,--dtype: <fp16|fp32>:
    The TensorFlow datatype used for calculations. Variables may be cast to a
    higher precision on a case-by-case basis for numerical stability.
    (default: 'fp32')
  --[no]enable_xla: Whether to enable XLA auto jit compilation
    (default: 'false')
  --[no]force_v2_in_keras_compile: Forces the use of run_distribued path even if
    notusing a `strategy`. This is not the same
    as`tf.distribute.OneDeviceStrategy`
    (default: 'false')
  -inter,--inter_op_parallelism_threads:
    Number of inter_op_parallelism_threads to use for CPU. See TensorFlow
    config.proto for details.
    (default: '0')
    (an integer)
  -intra,--intra_op_parallelism_threads:
    Number of intra_op_parallelism_threads to use for CPU. See TensorFlow
    config.proto for details.
    (default: '0')
    (an integer)
  -ls,--loss_scale:
    The amount to scale the loss by when the model is run. This can be an
    int/float
    or the string 'dynamic'. Before gradients are computed, the loss is
    multiplied
    by the loss scale, making all gradients loss_scale times larger. To adjust
    for
    this, gradients are divided by the loss scale before being applied to
    variables.
    This is mathematically equivalent to training without a loss scale, but the
    loss
    scale helps avoid some intermediate gradients from underflowing to zero. If
    not
    provided the default for fp16 is 128 and 1 for all other dtypes. The string
    'dynamic' can be used to dynamically determine the optimal loss scale during
    training, but currently this significantly slows down performance
  -mts,--max_train_steps:
    The model will stop training if the global_step reaches this value. If not
    set,
    training will run until the specified number of epochs have run as usual. It
    is
    generally recommended to set --train_epochs=1 when using this flag.
    (an integer)
  --num_packs:
    Sets `num_packs` in the cross device ops used in MirroredStrategy.  For
    details, see tf.distribute.NcclAllReduce.
    (default: '1')
    (an integer)
  -pgtc,--per_gpu_thread_count:
    The number of threads to use for GPU. Only valid when tf_gpu_thread_mode is
    not
    global.
    (default: '0')
    (an integer)
  --[no]tf_data_experimental_slack:
    Whether to enable tf.data's `experimental_slack` option.
    (default: 'false')
  -gt_mode,--tf_gpu_thread_mode:
    Whether and how the GPU device uses its own threadpool.
  -synth,--[no]use_synthetic_data:
    If set, use fake data (zeroes) instead of a real dataset. This mode is
    useful
    for performance debugging, as it removes input processing steps, but will
    not
    learn anything.
    (default: 'false')

tensorflow.python.ops.parallel_for.pfor:
  --[no]op_conversion_fallback_to_while_loop: If true, falls back to using a
    while loop for ops for which a converter is not defined.
    (default: 'false')

absl.flags:
  --flagfile: Insert flag definitions from the given file into the command line.
    (default: '')
  --undefok: comma-separated list of flag names that it is okay to specify on
    the command line even if the program does not define a flag with that name.
    IMPORTANT: flags in this list that have arguments MUST use the --flag=value
    format.
    (default: '')
